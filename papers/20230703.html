<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML 論文メモ</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="../index.html">ホーム</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <br>
        <section id="paper-info">
            <h2>論文情報</h2>
            <ul>
                <li><strong>タイトル ： </strong>Transformers learn to implement preconditioned gradient descent for in-context learning
                </li>
                <li><strong>著者 ： </strong>Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, Suvrit Sra
                </li>
                <li><strong>学会 ： </strong>???</li>
                <li><strong>出版年 ： </strong>2023</li>
                <li><strong>論文リンク ： </strong><a href="https://arxiv.org/abs/2306.00297">https://arxiv.org/abs/2306.00297</a></li>
            </ul>
        </section>
        <section id="paper-abstract">
            <h2>Abstract</h2>
            <ul>
                Transformerのコンテキスト内学習の驚異的な能力に動機づけられ,いくつかの研究がTransformerが勾配降下法のようなアルゴリズムを実装できることを示しています.これらの研究は,重みの慎重な構成によって,Transformerの複数の層が勾配降下の繰り返しをシミュレートするのに十分な表現力があることを示しています.表現力の問題を超えて,私たちは次のような質問をします:Transformerは,ランダムな問題のインスタンスを用いたトレーニングによって,そのようなアルゴリズムを学習できるでしょうか?ランダムな線形回帰のインスタンスで訓練された線形Transformerの損失ランドスケープの分析を通じて,この質問に対する最初の理論的進展を達成していることがわかりました.単一の注意層に対して,私たちはトレーニング目的の大域最小値が事前条件付き勾配降下の単一の繰り返しを実装することを証明します.特筆すべきは,Precondition行列は入力分布に適応するだけでなく,データの不適切さによって引き起こされる分散にも適応することです.kの注意層を持つTransformerに対して,私たちは目的関数のある臨界点が事前条件付き勾配降下のk回の繰り返しを実装することを証明します.私たちの結果は,Transformerのトレーニングによるアルゴリズムの学習に関する将来の理論的研究を求めています.
            </ul>
        </section>
        <section id="paper-summary">
            <h2>論文メモ</h2>
            <ul>
                <li>複数層からなる場合，事前学習の停留点（これらは上とは違って実際到達できるかは未証明）は，勾配降下法を複数ステップ実行するような解に対応し，その場合，各ステップ幅も自動調整され，そのうちいくつかはニュートン法に対応．</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
    </footer>
</body>
</html>
