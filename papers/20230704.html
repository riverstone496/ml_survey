<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML 論文メモ</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="../index.html">ホーム</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <br>
        <section id="paper-info">
            <h2>論文情報</h2>
            <ul>
                <li><strong>タイトル ： </strong>Surprises in High-Dimensional Ridgeless Least Squares Interpolation
                </li>
                <li><strong>著者 ： </strong>Trevor Hastie, Andrea Montanari, Saharon Rosset, Ryan J. Tibshirani
                </li>
                <li><strong>学会 ： </strong>???</li>
                <li><strong>出版年 ： </strong>2019</li>
                <li><strong>論文リンク ： </strong><a href="https://arxiv.org/abs/1903.08560">https://arxiv.org/abs/1903.08560</a></li>
            </ul>
        </section>
        <section id="paper-abstract">
            <h2>Abstract</h2>
            <ul>
                Interpolators -- 訓練損失がゼロになる推定器 -- は,主に最先端のニューラルネットワークがこのタイプのモデルであると考えられるため,機械学習で増加する注目を集めています.本論文では,高次元の最小二乗回帰における最小ℓ2ノルム("リッジレス")補間を研究します.特徴分布のモデルとして,2つの異なるモデルを考慮します:線形モデル,ここで特徴ベクトルxi∈ℝpは,独立同分布のエントリのベクトルに線形変換を適用して得られる,xi=Σ1/2zi(zi∈ℝpとする);および非線形モデル,ここで特徴ベクトルは,入力をランダムな1層のニューラルネットワークを通して得られる,xi=φ(Wzi)(zi∈ℝd, W∈ℝp×dは独立同分布のエントリの行列,φはWziの成分ごとに作用する活性化関数とする).私たちは,大規模なニューラルネットワークやカーネルマシンで観察されたいくつかの現象を,正確な定量的な方法で再現します,これには,予測リスクの二重降下の振る舞いや,過剰パラメータ化の潜在的な利点が含まれます.
            </ul>
        </section>
        <section id="paper-summary">
            <h2>論文メモ</h2>
            <ul>
                <li>線形回帰問題，行列分解，線形ニューラルネットワークにおいては，勾配降下法によって解の中でL2ノルムが最小である解を発見する．</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
    </footer>
</body>
</html>
