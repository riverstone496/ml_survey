<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML 論文メモ</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="../index.html">ホーム</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <br>
        <section id="paper-info">
            <h2>論文情報</h2>
            <ul>
                <li><strong>タイトル ： </strong>Why does deep and cheap learning work so well?
                </li>
                <li><strong>著者 ： </strong>Henry W. Lin, Max Tegmark, David Rolnick
                </li>
                <li><strong>学会 ： </strong></li>
                <li><strong>出版年 ： </strong>2017</li>
                <li><strong>論文リンク ： </strong><a href="https://arxiv.org/abs/1608.08225">https://arxiv.org/abs/1608.08225</a></li>
            </ul>
        </section>
        <section id="paper-abstract">
            <h2>Abstract</h2>
            <ul>
                私たちは,ディープラーニングの成功が数学だけでなく,物理学にも依存する可能性があることを示します: ニューラルネットワークが任意の関数をよく近似できることを保証するよく知られた数学的定理がありますが,実際に関心がある関数のクラスは,一般的なものよりも指数関数的に少ないパラメータで"安価な学習"を通じて頻繁に近似できます.私たちは,物理学で頻繁に遭遇する性質,例えば,対称性,局所性,合成性,多項式対数確率が,非常にシンプルなニューラルネットワークにどのように変換されるかを探求します.さらに,データを生成する統計的プロセスが物理学や機械学習で一般的なある階層的形式である場合,ディープニューラルネットワークは浅いものよりも効率的である可能性があると主張します.私たちはこれらの主張を情報理論を使用して形式化し,renormalization groupとの関係を議論します.私たちは,効率的な線形ディープネットワークが,効率の損失なしに浅いものによって正確に近似されない場合のさまざまな"非平坦化定理"を証明します.例えば,n変数を単一の隠れ層で2^nより少ないニューロンを使用して乗算することはできないことを示します.
            </ul>
        </section>
        <section id="paper-summary">
            <h2>論文メモ</h2>
            <ul>
                <li>多くのデータ生成過程は階層的でかつ構成的である．その生成過程はニューラルネットワークで十分近似できる．</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
    </footer>
</body>
</html>
