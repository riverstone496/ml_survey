<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML 論文メモ</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="../index.html">ホーム</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <br>
        <section id="paper-info">
            <h2>論文情報</h2>
            <ul>
                <li><strong>タイトル ： </strong>The loss surface of deep and wide neural networks
                </li>
                <li><strong>著者 ： </strong>Quynh Nguyen, Matthias Hein</li>
                <li><strong>学会 ： </strong>ICLR</li>
                <li><strong>出版年 ： </strong>2017</li>
                <li><strong>論文リンク ： </strong><a href="https://arxiv.org/abs/1704.08045">https://arxiv.org/abs/1704.08045</a></li>
            </ul>
        </section>
        <section id="paper-abstract">
            <h2>Abstract</h2>
            <ul>
                ディープニューラルネットワークの背後にある最適化問題は非常に非凸性が高いにもかかわらず,実際にはディープネットワークのトレーニングがサブオプティマルな点で立ち往生することなく可能であることが頻繁に観察されます.これは,すべての局所的な最小値がほぼ大域的に最適であるためだと主張されてきました.私たちは,これが(ほぼ)真実であることを示します.実際,ほぼすべての局所的な最小値が大域的に最適であることを示します.これは,ネットワークの1層の隠れユニットの数がトレーニングポイントの数よりも大きく,この層からのネットワーク構造がピラミッド型である場合の,二乗損失と解析的活性化関数を持つFully-connected ネットワークに対して成り立ちます.
            </ul>
        </section>
        <section id="paper-summary">
            <h2>論文メモ</h2>
            <ul>
                <li>ネットワークの1層の隠れユニットの数がトレーニングポイントの数より大きく,この層以降のネットワーク構造がピラミッド型である場合，global minimumとlocal minimumが一致することを示す．</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
    </footer>
</body>
</html>
