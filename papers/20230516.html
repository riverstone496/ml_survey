<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML 論文メモ</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="../index.html">ホーム</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <br>
        <section id="paper-info">
            <h2>論文情報</h2>
            <ul>
                <li><strong>タイトル ： </strong>Excess Risk of Two-Layer ReLU Neural Networks in Teacher-Student Settings and its Superiority to Kernel Methods</li>
                <li><strong>著者 ： </strong>Shunta Akiyama, Taiji Suzuki</li>
                <li><strong>学会 ： </strong></li>
                <li><strong>出版年 ： </strong>2022</li>
                <li><strong>論文リンク ： </strong><a href="https://arxiv.org/abs/2205.14818">https://arxiv.org/abs/2205.14818</a></li>
            </ul>
        </section>
        <section id="paper-abstract">
            <h2>Abstract</h2>
            <ul>
                ディープラーニングはさまざまなタスクで他の方法を上回っていますが,その理由を説明する理論的な枠組みはまだ完全に確立されていません.この問題に対処するために,私たちは教師-生徒回帰モデルにおける2層ReLUニューラルネットワークの過剰リスクを調査します.このモデルでは,生徒ネットワークはその出力を通じて未知の教師ネットワークを学習します.特に,教師ネットワークと同じ幅を持つ生徒ネットワークを考慮し,2つのフェーズで訓練します:まずノイズのある勾配降下法で,次にバニラの勾配降下法で学習を行います.私たちの結果は,生徒ネットワークが証明可能に近いグローバル最適解に到達し,最小最大最適率の意味で,NTKアプローチ,ランダム特徴モデル,および他のカーネル方法を含む任意のカーネル方法の推定器を上回ることを示しています.この優越性を引き起こす鍵となる概念は,ニューラルネットワークモデルの非凸性です.Loss landscapeが非常に非凸であるにもかかわらず,生徒ネットワークは適応的に教師のニューロンを学習します.
            </ul>
        </section>
        <section id="paper-summary">
            <h2>論文メモ</h2>
            <ul>
                <li>最適化を大域的探索フェーズと収束フェーズに分解して議論．</li>
                <li>探索フェーズではノイズ付きの最適化によって解を探索．最適解の周りでは損失関数は強凸のため，勾配法で線形収束させる．</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
    </footer>
</body>
</html>
