<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML 論文メモ</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="../index.html">ホーム</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <br>
        <section id="paper-info">
            <h2>論文情報</h2>
            <ul>
                <li><strong>タイトル ： </strong>The large learning rate phase of deep learning: the catapult mechanism
                </li>
                <li><strong>著者 ： </strong>Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, Guy Gur-Ari
                </li>
                <li><strong>学会 ： </strong>???</li>
                <li><strong>出版年 ： </strong>2021</li>
                <li><strong>論文リンク ： </strong><a href="https://arxiv.org/abs/2003.02218">https://arxiv.org/abs/2003.02218</a></li>
            </ul>
        </section>
        <section id="paper-abstract">
            <h2>Abstract</h2>
            <ul>
                初期学習率の選択は,ディープネットワークの性能に深い影響を与えることがあります.私たちは,訓練ダイナミクスが解けるニューラルネットワークのクラスを提示し,実際のディープラーニングの設定でその予測を実証的に確認します.ネットワークは,小さな学習率と大きな学習率で鮮明に異なる振る舞いを示します.これら2つの領域は,相転移によって分離されています.小さな学習率の段階では,訓練は無限に広いニューラルネットワークの既存の理論を使用して理解することができます.大きな学習率では,モデルは勾配降下ダイナミクスが平坦な最小値に収束するなど,質的に異なる現象を捉えます.私たちのモデルの主要な予測の1つは,大きくて安定した学習率の狭い範囲です.私たちは,私たちのモデルの予測と実際のディープラーニングの設定での訓練ダイナミクスとの間に良好な一致を見つけました.さらに,そのような設定での最適な性能は,しばしば大きな学習率の段階で見られることがわかりました.私たちは,異なる学習率で訓練されたモデルの特性についての私たちの結果が示唆を与えていると考えています.特に,それらは既存の広いニューラルネットワークの理論と,実践に関連する非線形で大きな学習率の訓練ダイナミクスとの間のギャップを埋めています.
            </ul>
        </section>
        <section id="paper-summary">
            <h2>論文メモ</h2>
            <ul>
                <li>大きな初期学習率を用いた時に発生するcatapult現象に関する論文．</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
    </footer>
</body>
</html>
