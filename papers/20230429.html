<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML 論文メモ</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="../index.html">ホーム</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <br>
        <section id="paper-info">
            <h2>論文情報</h2>
            <ul>
                <li><strong>タイトル ： </strong>Scaling Laws from the Data Manifold Dimension</li>
                <li><strong>著者 ： </strong>Utkarsh Sharma, Jared Kaplan</li>
                <li><strong>学会 ： </strong>JMLR</li>
                <li><strong>出版年 ： </strong>2022</li>
                <li><strong>論文リンク ： </strong><a href="https://jmlr.org/papers/v23/20-1111.html">https://jmlr.org/papers/v23/20-1111.html</a></li>
            </ul>
        </section>
        <section id="paper-abstract">
            <h2>Abstract</h2>
            <ul>
                データが豊富な場合,よく訓練されたニューラルネットワークによって達成されるテスト損失は,ネットワークパラメータNの数としての冪則L∝N^−αに従います.この経験的なScaling Lawは,さまざまなデータモダリティに対して成り立ち,多くのオーダーの大きさにわたって持続する可能性があります.このScaling Lawは,ニューラルモデルが本質的な次元dのデータ多様体に対して単に回帰を実行している場合に説明できます.このシンプルな理論は,クロスエントロピー損失と平均二乗誤差損失のためのスケーリング指数がα≈4/dであることを予測します.私たちは,ランダムな先生のネットワークの特性をダイヤルすることで,dとαのさまざまなものを研究できる先生/生徒のフレームワークで,本質的な次元とスケーリング指数を独立して測定することで,この理論を確認します.また,いくつかのデータセットでのCNN画像分類器やGPT型言語モデルでこの理論を実証します.
            </ul>
        </section>
        <section id="paper-summary">
            <h2>論文メモ</h2>
            <ul>
                <li>Scaling law</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
    </footer>
</body>
</html>
