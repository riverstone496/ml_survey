<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML 論文メモ</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="../index.html">ホーム</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <br>
        <section id="paper-info">
            <h2>論文情報</h2>
            <ul>
                <li><strong>タイトル ： </strong>Optimizing Millions of Hyperparameters by Implicit Differentiation</li>
                <li><strong>著者 ： </strong>Jonathan Lorraine, Paul Vicol, David Duvenaud</li>
                <li><strong>学会 ： </strong>AISTATS</li>
                <li><strong>出版年 ： </strong>2020</li>
                <li><strong>論文リンク ： </strong><a href="https://arxiv.org/abs/1911.02590">https://arxiv.org/abs/1911.02590</a></li>
            </ul>
        </section>
        <section id="paper-abstract">
            <h2>Abstract</h2>
            <ul>
                我々は, 陰関数定理 (IFT) と効率的な逆ヘッセ行列の近似を組み合わせた, 安価な勾配ベースのハイパーパラメータ最適化のためのアルゴリズムを提案します. 最適化を通じての微分とIFTとの関係に関する結果を示し, 我々のアルゴリズムの動機付けとします. 我々は, 提案されたアプローチを使用して, 数百万の重みと数百万のハイパーパラメータを持つ現代のネットワークアーキテクチャを訓練します. 例えば, 我々はデータ拡張ネットワークを学習します.ここで, すべての重みはValidationの性能のために調整されたハイパーパラメータであり,訓練例を増やして出力します. 我々のアプローチで重みとハイパーパラメータを共同で調整することは, 標準的な訓練よりもメモリや計算コストが数倍しかかかりません.
            </ul>
        </section>
        <section id="paper-summary">
            <h2>論文メモ</h2>
            <ul>
                <li>隠関数微分に基づくハイパーパラメータ最適化に関する論文．</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
    </footer>
</body>
</html>
