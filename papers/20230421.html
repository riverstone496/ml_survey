<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML 論文メモ</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="../index.html">ホーム</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <br>
        <section id="paper-info">
            <h2>論文情報</h2>
            <ul>
                <li><strong>タイトル ： </strong>LocoProp: Enhancing BackProp via Local Loss Optimization
                </li>
                <li><strong>著者 ： </strong>Ehsan Amid, Rohan Anil, Manfred K. Warmuth</li>
                <li><strong>学会 ： </strong>AISTATS</li>
                <li><strong>出版年 ： </strong>2022</li>
                <li><strong>論文リンク ： </strong><a href="https://arxiv.org/abs/2106.06199">https://arxiv.org/abs/2106.06199</a></li>
            </ul>
        </section>
        <section id="paper-abstract">
            <h2>Abstract</h2>
            <ul>
                二次の方法は,ディープニューラルネットワークを最適化するための最先端の性能を示しています.それにもかかわらず,一次の方法と比較して,大きなメモリ消費量と高い計算複雑性を持っているために,典型的な低予算のセットアップでの多様性を妨げています.この論文では,一次の最適化手法のみを利用しながら,二次の方法に近い性能を達成する多層ニューラルネットワークのための層ごとの損失構築の一般的なフレームワークを紹介します.私たちの方法論は,3つのコンポーネントである損失,ターゲット,および正則化の組み合わせに基づいており,各コンポーネントを変更することで新しい更新ルールが生まれます.私たちは,様々な転送関数の凸積分関数によって誘発される二乗損失と層ごとのBregmanダイバージェンスを使用した例を提供します.ベンチマークモデルとデータセットでの実験は,私たちの新しいアプローチの有効性を検証し,一次と二次の最適化手法の間のギャップを縮小します.
            </ul>
        </section>
        <section id="paper-summary">
            <h2>論文メモ</h2>
            <ul>
                <li>層ごとに勾配法を行うことで，実質的に二次最適化を行うことができることを示す．反復数無限大ではFOOFと一致する．</li>
                <li>KFACを層ごとの自然勾配法として新たな定式化をしている．</li>
                <li>層ごとの勾配法によって必ず損失関数が下がるのかは不明．</li>
                <li>pre-activationに対して勾配法を行うのと，post-activationに対して勾配法を行うのとではどちらの方が性能がいいのかはfuture-workとしている．</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
    </footer>
</body>
</html>
