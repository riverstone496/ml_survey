<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML 論文メモ</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="../index.html">ホーム</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <br>
        <section id="paper-info">
            <h2>論文情報</h2>
            <ul>
                <li><strong>タイトル ： </strong>On Random Weights and Unsupervised Feature Learning</li>
                <li><strong>学会 ： </strong>ICML</li>
                <li><strong>出版年 ： </strong>2011</li>
                <li><strong>論文リンク ： </strong><a href="https://www.saxelab.org/assets/papers/Saxe2010.pdf">https://www.saxelab.org/assets/papers/Saxe2010.pdf</a></li>
            </ul>
        </section>
        <section id="paper-summary">
            <h2>論文メモ</h2>
            <ul>
                <li>NNはランダムに初期化されたモデルにおいても既に良い画像特徴量が捉えられている可能性が指摘されている．この論文ではその優れた画像特徴量として，「周波数選択性」と「不変性」を挙げている．</li>
                <li>ランダム結合による畳み込み+poolingは，ガボール的な特徴に選択的に反応することを数学的に示している．</li>
                <li>CNNの性能が良いのは学習システムではなく，アーキテクチャの形状がもつ対称性に由来していると唱える初期の論文．</li>
                <li>ネットワークの構造が適当な場合は事前学習を行なったとしても意味がなく，事前学習を行なっていないモデルに性能で負けることすらある．そのためネットワークの構造の探索が重要である．この論文では，ランダム初期化の重みから学習を行ったモデルと，事前学習済みモデルのFinetunin後の性能に強い相関があることを示す．これにより，ランダム初期化されたネットワークの性能によってモデルの構造を決定することで，事前学習済みモデルのfinetuning性能を上げることができることを示す．</li>
                <li>学習を行わなくても，モデルが認識に有効な画像特徴量を既に複数獲得しているというのは面白い．</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
    </footer>
</body>
</html>
