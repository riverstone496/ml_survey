<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML 論文メモ</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="../index.html">ホーム</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <br>
        <section id="paper-info">
            <h2>論文情報</h2>
            <ul>
                <li><strong>タイトル ： </strong>Putting An End to End-to-End: Gradient-Isolated Learning of Representations</li>
                <li><strong>著者 ： </strong>Sindy Löwe, Peter O'Connor, Bastiaan S. Veeling</li>
                <li><strong>学会 ： </strong>NeurIPS</li>
                <li><strong>出版年 ： </strong>2019</li>
                <li><strong>論文リンク ： </strong><a href="https://arxiv.org/abs/1905.11786">https://arxiv.org/abs/1905.11786</a></li>
            </ul>
        </section>
        <section id="paper-abstract">
            <h2>Abstract</h2>
            <ul>
                私たちは,ラベルやエンドツーエンドのbackpropを必要とせず,代わりにデータの自然な順序を利用するローカルな自己教師付き表現学習のための新しいディープラーニング方法を提案します.生物学的なニューラルネットワークがグローバルなエラーシグナルをバックプロパゲートせずに学習することができるという観察に触発され,私たちはディープニューラルネットワークを勾配が孤立したモジュールのスタックに分割します.各モジュールは,Oord et al. [2018]のInfoNCEバウンドを使用して,その入力の情報を最大限に保持するように訓練されます.この貪欲な訓練にもかかわらず,各モジュールはその前のモジュールの出力を改善し,トップモジュールによって作成された表現は,オーディオとビジュアルのドメインの下流の分類タスクにおいて非常に競争力のある結果をもたらします.この提案により,モジュールを非同期に最適化することができ,ラベルのないデータセット上で非常に深いニューラルネットワークの大規模な分散トレーニングが可能となります.
            </ul>
        </section>
        <section id="paper-summary">
            <h2>論文メモ</h2>
            <ul>
                <li>ネットワークの各段階において、局所的に表現を学習するGreedy Infomaxを提案．</li>
                <li>各モジュールは局所的な対照損失を用いてgreedyに学習される.</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
    </footer>
</body>
</html>
