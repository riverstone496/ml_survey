<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML 論文メモ</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="../index.html">ホーム</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <br>
        <section id="paper-info">
            <h2>論文情報</h2>
            <ul>
                <li><strong>タイトル ： </strong>The Implicit Bias of Gradient Descent on Separable Data
                </li>
                <li><strong>著者 ： </strong>Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, Nathan Srebro
                </li>
                <li><strong>学会 ： </strong>JMLR</li>
                <li><strong>出版年 ： </strong>2018</li>
                <li><strong>論文リンク ： </strong><a href="https://arxiv.org/abs/1710.10345">https://arxiv.org/abs/1710.10345</a></li>
            </ul>
        </section>
        <section id="paper-abstract">
            <h2>Abstract</h2>
            <ul>
                私たちは,線形に分離可能なデータセット上での均一な線形予測器を持つ,正則化されていないロジスティック回帰の問題に対する勾配降下法を調査します.予測器がmax-margin(ハードマージンSVM)の解の方向に収束することを示します.この結果は,無限大での下限を持つ他の単調減少損失関数,多クラスの問題,および特定の制限された設定でのディープネットワークの重み層の学習にも一般化されます.さらに,この収束が非常に遅く,損失自体の収束の対数のみであることを示します.これは,訓練誤差がゼロで,訓練損失が非常に小さい場合,さらに,検証損失が増加する場合であっても,ロジスティックまたはクロスエントロピー損失の最適化を続ける利点を説明するのに役立ちます.私たちの方法は,より複雑なモデルや他の最適化方法での暗黙の正則化を理解するのにも役立ちます.
            </ul>
        </section>
        <section id="paper-summary">
            <h2>論文メモ</h2>
            <ul>
                <li>多層ニューラルネットワークにおいては，正規化マージン最大化を達成する．</li>
                <li>正規化マージン最大化は，ノルム最小化と似た意味を持つ．（分類面とそれに一番近い点までの距離が最大化するような重みと，そのノルムを正規化した上で一致．</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
    </footer>
</body>
</html>
