<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML 論文メモ</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="../index.html">ホーム</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <br>
        <section id="paper-info">
            <h2>論文情報</h2>
            <ul>
                <li><strong>タイトル ： </strong>A Theoretical Framework for Target Propagation
                </li>
                <li><strong>著者 ： </strong>Alexander Meulemans, Francesco S. Carzaniga, Johan A.K. Suykens, João Sacramento, Benjamin F. Grewe</li>
                <li><strong>学会 ： </strong>Neurips</li>
                <li><strong>出版年 ： </strong>2020</li>
                <li><strong>論文リンク ： </strong><a href="https://arxiv.org/abs/2006.14331">https://arxiv.org/abs/2006.14331</a></li>
            </ul>
        </section>
        <section id="paper-abstract">
            <h2>Abstract</h2>
            <ul>
                ディープラーニング,つまり脳に触発されたAIの形態の成功は,脳が同様に複数のニューロンの層を通じてどのように学習するかを理解することへの関心を引き起こしています.しかし,生物学的に妥当な学習アルゴリズムの大部分はまだバックプロパゲーション(BP)の性能に達しておらず,また強固な理論的基盤に基づいているわけではありません.ここでは,数学的最適化の観点からBPの代わりとして人気があるがまだ完全に理解されていないターゲット伝播(TP)を分析します.私たちの理論は,TPがガウス-ニュートン最適化と密接に関連しており,BPとは大きく異なることを示しています.さらに,我々の分析は,非可逆的なニューラルネットワークの現実的なシナリオにおける,TPのよく知られたバリアントである差分ターゲット伝播(DTP)の根本的な制約を明らかにしています.我々は,フィードバック重みのトレーニングを改善する新しい再構築損失を通じて,この問題への最初の解決策を提供すると同時に,出力から各隠れ層への直接のフィードバック接続を許可することでアーキテクチャの柔軟性を導入します.我々の理論は,DTPと比較して,性能の大幅な向上と前方重みの更新と損失勾配の整合性の向上を示す実験結果によって裏付けられています.
            </ul>
        </section>
        <section id="paper-summary">
            <h2>論文メモ</h2>
            <ul>
                <li>Target Propagationのアルゴリズムがガウスニュートン法を近似していることを示す．</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
    </footer>
</body>
</html>
