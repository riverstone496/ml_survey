<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML 論文メモ</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="../index.html">ホーム</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <br>
        <section id="paper-info">
            <h2>論文情報</h2>
            <ul>
                <li><strong>タイトル ： </strong>Why are Adaptive Methods Good for Attention Models?</li>
                <li><strong>著者 ： </strong>Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi, Sanjiv Kumar, Suvrit Sra</li>
                <li><strong>学会 ： </strong>NeurIPS</li>
                <li><strong>出版年 ： </strong>2020</li>
                <li><strong>論文リンク ： </strong><a href="https://arxiv.org/abs/1912.03194">https://arxiv.org/abs/1912.03194</a></li>
            </ul>
        </section>
        <section id="paper-abstract">
            <h2>Abstract</h2>
            <ul>
                確率的勾配降下法(SGD)はまだディープラーニングにおいて典型的なアルゴリズムである一方,Clipped SGD/Adamのような適応的な方法は,Attentionモデルのような重要なタスクでSGDを上回ることが観察されています.SGDが適応的な方法と比較して不十分に機能する設定は,まだよく理解されていません.本論文では,確率的勾配のノイズのheavy tailed分布がSGDの不十分な性能の一因であることを,経験的および理論的に示します.私たちは,heavy tailedノイズの下での適応的勾配方法の最初の上限および下限の収束境界を提供します.さらに,勾配のクリッピングがheavy tailedな勾配ノイズに対処する上で重要な役割を果たす方法を示します.その後,実際にクリッピングを適用する方法を示すために,適応的な座標ごとのクリッピングアルゴリズム(ACClip)を開発し,BERTの事前学習および微調整タスクでのその優れた性能を示します.
            </ul>
        </section>
        <section id="paper-summary">
            <h2>論文メモ</h2>
            <ul>
                <li>Transformerの勾配ノイズはheavy-tailedな確率分布になることを指摘．これによってTransformerの学習においてAdamがSGDに比べて優位に収束が早くなっていると指摘している．</li>
                <li>heavy-tailedな勾配ノイズを抑えるには要素ごとのGradient Clippingが有効であると指摘する．</li>
                <li>勾配ノイズがheavy-tailedになるのは，データセットとモデルの構造の両方が原因であるとする．</li>
            </ul>
        </section>
    </main>
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
    </footer>
</body>
</html>
