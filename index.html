<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML論文メモ</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="#tag1">ML Theoretical Study</a></li>
                <li><a href="#tag2">ML Empirical Stury</a></li>
                <li><a href="#tag3">ML Algorithm</a></li>
                <li><a href="#tag4">Future Reading List</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section id="tag1">
            <h2>Learning Theory</h2>
            <ul>
                <li><a href="papers/20231004.html">A Kernel-Based View of Language Model Fine-Tuning
                </a></li>
                <li><a href="papers/20231003.html">Adaptive Gradient Methods at the Edge of Stability
                </a></li>
                <li><a href="papers/20231002.html">Small-scale proxies for large-scale Transformer training instabilities
                </a></li>
                <li><a href="papers/20230707.html">On the interplay between noise and curvature and its effect on optimization and generalization
                </a></li>
                <li><a href="papers/20230705.html">The Implicit Bias of Gradient Descent on Separable Data
                </a></li>
                <li><a href="papers/20230704.html">Surprises in High-Dimensional Ridgeless Least Squares Interpolation
                </a></li>
                <li><a href="papers/20230701.html">The large learning rate phase of deep learning: the catapult mechanism
                </a></li>
                <li><a href="papers/20230702.html">One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention
                </a></li>
                <li><a href="papers/20230703.html">Transformers learn to implement preconditioned gradient descent for in-context learning
                </a></li>
                <li><a href="papers/20230526.html">Kernel and Rich Regimes in Overparametrized Models
                </a></li>
                <li><a href="papers/20230522.html">Why are Adaptive Methods Good for Attention Models?
                </a></li>
                <li><a href="papers/20230515.html">Exact natural gradient in deep linear networks and its application to the nonlinear case
                </a></li>
                <li><a href="papers/20230511.html">Understanding AdamW through Proximal Methods and Scale-Freeness
                </a></li>
                <li><a href="papers/20230508.html">The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers
                </a></li>
                <li><a href="papers/20230507.html">Noise Is Not the Main Factor Behind the Gap Between SGD and Adam on Transformers, but Sign Descent Might Be
                </a></li>
                <li><a href="papers/20230429.html">Scaling Laws from the Data Manifold Dimension
                </a></li>
                <li><a href="papers/20230428.html">Dynamics of Learning in MLP: Natural Gradient and Singularity Revisited
                </a></li>
                <li><a href="papers/20230423.html">Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective
                </a></li>
                <li><a href="papers/20230420.html">Train longer, generalize better: closing the generalization gap in large batch training
                </a></li>
                <li><a href="papers/20230417.html">Omnigrok: Grokking Beyond Algorithmic Data
                </a></li>
                <li><a href="papers/20230416.html">The loss surface of deep and wide neural networks
                </a></li>
                <li><a href="papers/20230415.html">A Theoretical Framework for Target Propagation
                </a></li>
                <li><a href="papers/20230412.html">Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks
                </a></li>
            </ul>
        </section>
        <section id="tag2">
            <h2>Model Architecture</h2>
            <ul>
                <li><a href="papers/20230517.html">Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification
                </a></li>
                <li><a href="papers/20230706.html">Why does deep and cheap learning work so well?
                </a></li>
                <li><a href="papers/20230525.html">Are Convolutional Neural Networks or Transformers more like human vision?
                </a></li>
                <li><a href="papers/20230523.html">On Random Weights and Unsupervised Feature Learning
                </a></li>
                <li><a href="papers/20230424.html">Mean Field Residual Networks: On the Edge of Chaos
                </a></li>
            </ul>
        </section>
        <section id="tag3">
            <h2>Learning Algorithm</h2>
            <ul>
                <li><a href="papers/20231006.html">Deep Unlearning via Randomized Conditionally Independent Hessians
                </a></li>
                <li><a href="papers/20230425.html">Towards Scaling Difference Target Propagation by Learning Backprop Targets
                </a></li>
                <li><a href="papers/20230427.html">Putting An End to End-to-End: Gradient-Isolated Learning of Representations
                </a></li>
                <li><a href="papers/20230422.html">LoCo: Local Contrastive Representation Learning
                </a></li>
                <li><a href="papers/20230421.html">LocoProp: Enhancing BackProp via Local Loss Optimization
                </a></li>
                <li><a href="papers/20230414.html">Scaling Forward Gradient With Local Losses
                </a></li>
            </ul>
        </section>
        <section id="tag4">
            <h2>Future Reading List</h2>
            <ul>
                <li><a href="papers/20230413.html">Intrinsic dimension of data representations in deep neural networks
                </a></li>
                <li><a href="papers/20230419.html">Deep Learning without Shortcuts: Shaping the Kernel with Tailored Rectifiers
                </a></li>
                <li><a href="https://arxiv.org/abs/2006.16840">Guided Learning of Nonconvex Models through Successive Functional Gradient Optimization
                </a></li>
                <li><a href="http://robotics.stanford.edu/~ang/papers/nipsdlufl10-RandomWeights.pdf">On Random Weights and Unsupervised Feature Learning
                </a></li>
                <li><a href="https://arxiv.org/abs/1802.06175">An Alternative View: When Does SGD Escape Local Minima?
                </a></li>
                <li><a href="https://arxiv.org/abs/1910.05992">Pathological spectra of the Fisher information metric and its variants in deep neural networks
                </a></li>
                <li><a href="https://arxiv.org/abs/2002.02923">Geometric Dataset Distances via Optimal Transport
                </a></li>
                <li><a href="papers/20230418.html">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima
                </a></li>
                <li><a href="papers/20230426.html">On the Connection between Local Attention and Dynamic Depth-wise Convolution
                </a></li>
                <li><a href="papers/20230506.html">Convergence of Gradient Descent on Separable Data
                </a></li>
                <li><a href="papers/20230509.html">On the Origin of Implicit Regularization in Stochastic Gradient Descent
                </a></li>
                <li><a href="papers/20230510.html">A new regret analysis for Adam-type algorithms
                </a></li>
                <li><a href="papers/20230512.html">Aggregated Momentum: Stability Through Passive Damping
                </a></li>
                <li><a href="papers/20230513.html">On the generalization benefit of noise in stochastic gradient descent
                </a></li>
                <li><a href="papers/20230514.html">The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning
                </a></li>
                <li><a href="papers/20230516.html">Excess Risk of Two-Layer ReLU Neural Networks in Teacher-Student Settings and its Superiority to Kernel Methods
                </a></li>
                <li><a href="papers/20230518.html">Picking Winning Tickets Before Training by Preserving Gradient Flow
                </a></li>
                <li><a href="papers/20230519.html">Advancing Model Pruning via Bi-level Optimization
                </a></li>
                <li><a href="papers/20230520.html">Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve
                </a></li>
                <li><a href="papers/20230521.html">BaLeNAS: Differentiable Architecture Search via the Bayesian Learning Rule
                </a></li>
                <li><a href="papers/20230524.html">Learning from Randomly Initialized Neural Network Features
                </a></li>
                <li><a href="papers/20231005.html">Why (and When) does Local SGD Generalize Better than SGD?
                </a></li>
            </ul>
        </section>
        
    </main>
    
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
        <br>
        <form method="get" action="http://www.google.co.jp/search" target="_blank">
            <input type="text" name="q" size="30" maxlength="255" value="">
            <input type="submit" name="btn" size="7" value="search">
            <input type="hidden" name="hl" value="ja">
            <input type="hidden" name="sitesearch" value="https://riverstone496.github.io/ml_survey/">
        </form>
    </footer>
</body>
</html>