<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ML論文メモ</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>ML 論文メモ</h1>
        <nav>
            <ul>
                <li><a href="#tag1">ML Theory</a></li>
                <li><a href="#tag2">ML Algorithm</a></li>
                <li><a href="#tag3">Future Reading List</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section id="tag1">
            <h2>ML Theory</h2>
            <ul>
                <li><a href="papers/20230424.html">Mean Field Residual Networks: On the Edge of Chaos
                </a></li>
                <li><a href="papers/20230423.html">Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective
                </a></li>
                <li><a href="papers/20230420.html">Train longer, generalize better: closing the generalization gap in large batch training
                </a></li>
                <li><a href="papers/20230417.html">Omnigrok: Grokking Beyond Algorithmic Data
                </a></li>
                <li><a href="papers/20230416.html">The loss surface of deep and wide neural networks
                </a></li>
                <li><a href="papers/20230415.html">A Theoretical Framework for Target Propagation
                </a></li>
                <li><a href="papers/20230412.html">Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks
                </a></li>
            </ul>
        </section>
        <section id="tag2">
            <h2>ML Algorithm</h2>
            <ul>
                <li><a href="papers/20230425.html">Towards Scaling Difference Target Propagation by Learning Backprop Targets
                </a></li>
                <li><a href="papers/20230422.html">LoCo: Local Contrastive Representation Learning
                </a></li>
                <li><a href="papers/20230421.html">LocoProp: Enhancing BackProp via Local Loss Optimization
                </a></li>
                <li><a href="papers/20230414.html">Scaling Forward Gradient With Local Losses
                </a></li>
            </ul>
        </section>
        <section id="tag3">
            <h2>Future Reading List</h2>
            <ul>
                <li><a href="papers/20230413.html">Intrinsic dimension of data representations in deep neural networks
                </a></li>
                <li><a href="papers/20230419.html">Deep Learning without Shortcuts: Shaping the Kernel with Tailored Rectifiers
                </a></li>
                <li><a href="https://arxiv.org/abs/2006.16840">Guided Learning of Nonconvex Models through Successive Functional Gradient Optimization
                </a></li>
                <li><a href="http://robotics.stanford.edu/~ang/papers/nipsdlufl10-RandomWeights.pdf">On Random Weights and Unsupervised Feature Learning
                </a></li>
                <li><a href="https://arxiv.org/abs/1802.06175">An Alternative View: When Does SGD Escape Local Minima?
                </a></li>
                <li><a href="https://proceedings.neurips.cc/paper/2018/hash/7f018eb7b301a66658931cb8a93fd6e8-Abstract.html">Exact natural gradient in deep linear networks and its application to the nonlinear case
                </a></li>
                <li><a href="https://bsi-ni.brain.riken.jp/database/file/349/349.pdf">Dynamics of Learning in MLP: Natural Gradient and Singularity Revisited
                </a></li>
                <li><a href="https://arxiv.org/abs/1910.05992">Pathological spectra of the Fisher information metric and its variants in deep neural networks
                </a></li>
                <li><a href="https://jmlr.org/papers/v23/20-1111.html">Scaling Laws from the Data Manifold Dimension
                </a></li>
                <li><a href="https://arxiv.org/abs/2002.02923">Geometric Dataset Distances via Optimal Transport
                </a></li>
                <li><a href="https://arxiv.org/abs/2002.09277">Kernel and Rich Regimes in Overparametrized Models
                </a> (対角線形ネットワーク)</li>
                <li><a href="papers/20230418.html">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima
                </a></li>
                <li><a href="papers/20230426.html">On the Connection between Local Attention and Dynamic Depth-wise Convolution
                </a></li>
                <li><a href="papers/20230427.html">Putting An End to End-to-End: Gradient-Isolated Learning of Representations
                </a></li>
                <li><a href="papers/20230510.html">A new regret analysis for Adam-type algorithms
                </a></li>
                <li><a href="papers/20230511.html">Understanding AdamW through Proximal Methods and Scale-Freeness
                </a></li>
                <li><a href="papers/20230512.html">Aggregated Momentum: Stability Through Passive Damping
                </a></li>
                <li><a href="papers/20230513.html">On the generalization benefit of noise in stochastic gradient descent
                </a></li>
                <li><a href="papers/20230514.html">The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning
                </a></li>
            </ul>
        </section>
        
    </main>
    
    <footer>
        <p>&copy; 2023 ML論文メモ</p>
        <br>
        <form method="get" action="http://www.google.co.jp/search" target="_blank">
            <input type="text" name="q" size="30" maxlength="255" value="">
            <input type="submit" name="btn" size="7" value="search">
            <input type="hidden" name="hl" value="ja">
            <input type="hidden" name="sitesearch" value="https://riverstone496.github.io/ml_survey/">
        </form>
    </footer>
</body>
</html>